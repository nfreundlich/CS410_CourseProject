import numpy as np
from scipy.sparse import csr_matrix


class EM:
    def __init__(self, dump_path = "../tests/data/em_01/"):
        self.dump_path = dump_path

        # Parameters for EM execution
        self.max_iter = 50
        self.lambda_background = 0.7
        self.dist_threshold = 1e-6

        # Parameters for Data size evaluation
        self.m = 0
        self.nw = 0
        self.na = 0

        # Parameters for EM internals
        self.reviews = np.load(self.dump_path + "Reviews.npy")
        self.hidden_parameters = np.load(self.dump_path + "HP.npy")
        self.hidden_parameters_background = np.load(self.dump_path + "HPB.npy")
        self.pi = np.load(self.dump_path + "PI.npy")
        self.topic_model = np.load(self.dump_path + 'TopicModel.npy').item()
        self.background_probability = np.load(self.dump_path + 'BackgroundProbability.npy').item()

        # Temporary parameters for matrix testing
        # TODO: update original parameters for this usage
        self.aspects_map = {}
        self.words_map = {}
        self.reviews_matrix = np.array([])
        self.pi_matrix = np.array([])
        self.topic_model_matrix = ()

    def em_e_step_dense(self):
        """
        E-Step of EM algo, as implemented by ***Santu***.
        Compute HP and BHP.

        Input:
            REVIEWS
            TOPIC MODEL
            PI (probability of choosing a FLM to generate a word)
            BACKGROUND PROBABILITY (BP) //scalar
            LAMBDA BACKGROUND (lambdaB) //scalar
            HP
            HPB

        Output:
            Updated HP
            Updated HPB

        Pseudo-code
        for each REVIEW (R):
            for each LINE in R (L):
                for each WORD in L (W):
                    - initialize sum of probabilities W in all topic models (S) to 0
                    - for each ASPECT in TOPIC_MODEL (A):
                        S += PI[R][L][A] * TOPIC_MODEL[A][W]
                        //prob. that line L of review R was generated by aspect A * prob. of word W in aspect A
                    - for each ASPECT in TOPIC_MODEL (A):
                        HP[R][L][W][A] = PI[R][L][A] * TOPIC_MODEL[A][W] / S
                        //compute prob that word W of line L (the "sentence") of review R is generated from aspect A
                    - HPB[R][L][W] = (lambdaB * BP[W]) / (lambdaB * BP[W] + ((1 - lambdaB) * S))
                        //compute ptob that word W of line L of review R generated from BLM

        :return:
        """
        for reviewNum in range(0, len(self.reviews)):
            for lineNum in range(0, len(self.reviews[reviewNum])):
                for word in self.reviews[reviewNum][lineNum]:
                    mysum = 0
                    for aspect in self.topic_model:
                        mysum += self.pi[reviewNum][lineNum][aspect] * self.topic_model[aspect][word]
                    for aspect in self.topic_model:
                        self.hidden_parameters[reviewNum][lineNum][word][aspect] = self.pi[reviewNum][lineNum][aspect] * self.topic_model[aspect][
                            word] / mysum

                        self.hidden_parameters_background[reviewNum][lineNum][word] = (self.lambda_background * self.background_probability[word]) / (self.lambda_background * self.background_probability[word] + ((1 - self.lambda_background) * mysum))

        np.save(self.dump_path + "MY_HP_Updated", self.hidden_parameters)
        np.save(self.dump_path + "MY_HPB_updated", self.hidden_parameters_background)

    def em_prepare_data_for_testing(self):
        """
        Prepare data for testing vectorised solution.
        Want to convert the photo of Santu's data for vectorised needs.
        :return:
        """
        m = 0   # number of sentences (lines) in all reviews - 8799
        nw = 0  # number of words in vocabulary - 7266
        na = 0  # number of aspects - 9

        for reviewNum in range(0, len(self.reviews)):
            for lineNum in range(0, len(self.reviews[reviewNum])):
                m += 1
        for aspect in self.topic_model:
            na += 1

        words_dict = {}
        for aspect in self.topic_model.keys():
            print(aspect, len(self.topic_model[aspect]))
            for word in self.topic_model[aspect]:
                words_dict[word] = True
        nw = len(words_dict.keys()) # 7266
        word_list = sorted(words_dict.keys())
        words_map = {}
        for word_id in range(0, len(word_list)):
            words_map[word_list[word_id]] = word_id

        print("m", "nw", "na")
        print(m, nw, na)

        # initialize reviews with zeros
        reviews_matrix = np.zeros(m * nw).reshape(m, nw)

        # construct the review matrix with count values for each words
        section_id = 0
        for reviewNum in range(0, len(self.reviews)):
            for lineNum in range(0, len(self.reviews[reviewNum])):
                for word in self.reviews[reviewNum][lineNum]:
                    reviews_matrix[section_id][words_map[word]] = self.reviews[reviewNum][lineNum][word]
                section_id += 1
        print("number of sections")
        print(section_id)

        # check first line
        for i in range(0, len(reviews_matrix[0])):
            if reviews_matrix[0][i] != 0:
                print(i, reviews_matrix[0][i], word_list[i])

        # construct the aspect map
        current_aspect = 0
        aspects_map = {}
        for one_aspect in sorted(self.pi[0][0].keys()):
            aspects_map[one_aspect] = current_aspect
            current_aspect += 1

        # initialize pi with zeros
        # pi_matrix = np.random.dirichlet(np.ones(m), na).transpose()
        pi_matrix = np.zeros(m * na).reshape(m, na)
        section_id = 0
        pi_matrix = np.zeros(m * na).reshape(m, na)
        for reviewNum in range(0, len(self.reviews)):
            for lineNum in range(0, len(self.reviews[reviewNum])):
                for aspect in self.pi[reviewNum][lineNum]:
                    pi_matrix[section_id][aspects_map[aspect]] = self.pi[reviewNum][lineNum][aspect]
                section_id += 1

        # initialize topic model with zeros
        topic_model_matrix = np.zeros(nw * na).reshape(nw, na)
        for aspect in self.topic_model:
            for word in self.topic_model[aspect]:
                topic_model_matrix[words_map[word]][aspects_map[aspect]] = self.topic_model[aspect][word]

        # update class parameters with matrices
        # TODO: clean this up to use only one set of input data
        self.reviews_matrix = reviews_matrix
        self.topic_model_matrix = topic_model_matrix
        self.pi_matrix = pi_matrix
        self.m = m
        self.nw = nw
        self.na = na
        self.aspects_map = aspects_map
        self.words_map = words_map
        self.words_list = word_list

    def em_prepare_data_for_testing_dummy(self):
        """
        Prepare dummy data for implementation purposes.
        TODO: delete this
        :return:
        """
        self.m = 2
        self.nw = 3
        self.na = 4

        self.reviews_matrix = np.random.randint(0, 2, self.m * self.nw).reshape(self.m, self.nw)
        self.topic_model = np.random.random(self.nw * self.na).reshape(self.nw, self.na)
        self.pi_matrix = np.random.dirichlet(np.ones(self.na), self.m)

    def em_e_step_sparse(self):
        """
        Vectorised E-step of EM.
        Needs intensive testing, at this stage.

        :return:
        """
        # Notation:
        # nw = number of words in vocabulary
        # m  = number of sentences (lines) in all reviews
        # na = number of aspects

        m   = self.m # 2 # number of sentences (lines) in all reviews
        nw  = self.nw # 3 # number of words in vocabulary
        na  = self.na #4 # number of aspects

        #Initialize reviews matrix:
        #Sentence/Word | word 1 ... ... ... ... word nw
        #---------------------------------------------------
        #Sentence 1    | count(s_1,w_1) ... ...  count(s_1, w_nw)
        #Sentence 2    | count(s_2,w_2) ... ...  count(s_2, w_nw)
        #...    ...     ... ...     ...     ...     ...
        #Sentence m    | count(s_m, w_1)... ...  count(s_m, w_nw)

        # random review
        # reviews = csr_matrix(np.random.randint(0, 3, m * nw).reshape(m, nw))
        reviews = self.reviews_matrix # np.random.randint(0, 2, m * nw).reshape(m, nw)
        print("reviews")
        print(reviews)

        # Compute binary reviews matrix (1 if word in sentence, 0 if not) (same dimensions as reviews)
        reviews_binary = np.where(reviews > 0, 1, 0)
        print("reviews_binary")
        print(reviews_binary)

        # Compute sparse matrix
        reviews = csr_matrix(reviews)
        reviews_binary = csr_matrix(reviews_binary)

        #Initialize topic model
        #Word/Aspect    | aspect 1   ...     ...     aspect na
        #-----------------------------------------------------
        #word 1         | tm(w1,a1) ...      ...    tm(w1, a_na)
        #...        ...             ....            ...     ...
        #word nw        | tm(w_nw, a_na) ... ....   tm(w_na, a_na)

        # random topic model
        # topic_model = csr_matrix(np.random.randint(0, 3, nw * na).reshape(nw, na))
        topic_model = self.topic_model_matrix # np.random.random(nw * na).reshape(nw, na)
        print("topic_model")
        print(topic_model)

        #Initialize pi
        #Sentence/Aspect | aspect 1 ...      ...    aspect na
        #-----------------------------------------------------
        #Sentence 1      | pi(s1,a1)  ...   ...     pi(s1, a_na)
        # ...        ...             ....            ...     ...
        #Sentence m      | pi(sm,a1)  ...   ...     pi(sm, a_na)
        pi = self.pi_matrix # np.random.dirichlet(np.ones(na), m)
        print("pi")
        print(pi)
        print("sum of pi for each sentence")
        print(pi.sum(axis=1))

        #Initialize hidden_parameters FOR ONE SENTENCE
        #Sentence/Word | word 1 ... ... ... ... word nw
        #---------------------------------------------------
        #Sentence 1    | 0.0        ...     ...   0.0
        #Sentence 2    | 0.0    ...         ...   0.0
        #...    ...     ... ...     ...     ...   ...
        #Sentence m    | 0.0 ...             ...  0.0
        hidden_parameters = []
        hidden_parameters_one_sentence = np.zeros(m * nw).reshape(m, nw)
        for sentence in range(0, m):
            hidden_parameters.append(hidden_parameters_one_sentence)

        print("hidden_parameters")
        print(hidden_parameters[0])

        #TODO: treat multiple sentences
        print(100 * '*', "Start one sentence.")
        for sentence in range(0, 1): # TODO: replace with 'm' (now not needed)
            # Compute topic model for sentence as review_binary[sentence_s]^T * topic_model
            # sentence = 0
            # topic_model_sentence = reviews_binary.T[:,sentence].reshape(nw,1) * topic_model -- method with dummy data
            topic_model_sentence = reviews_binary[sentence].reshape(nw,1).multiply(topic_model)
            print("topic_model_sentence: " + str(sentence))
            print(topic_model_sentence)

            # Compute sum of review * topic_model for sentence_s
            # sentence_sum = pi[sentence].reshape(1,na).dot(topic_model_sentence.transpose()) -- method with dummy data
            sentence_sum = topic_model_sentence.dot(pi[sentence])
            print("snp.where(sentence_sum > 0)")
            print(np.where(sentence_sum > 0))

            # We will have 0 values for sentence_sum, for missing words
            # To avoid division by 0, sentence_sum(word) = 1
            sentence_sum = np.where(sentence_sum == 0, 1, sentence_sum)
            print("np.where(sentence_sum < 1), identity for multiplication/division")
            print(np.where(sentence_sum < 1)) # only computed sums

            # Compute hidden_parameters for sentence_s
            # hidden_parameters_sentence = (topic_model_sentence * pi[sentence]).transpose() / sentence_sum -- method with dummy data
            hidden_parameters_sentence = ((topic_model_sentence.multiply(pi[sentence])).T / sentence_sum).T # TODO: not optimal! Redox!
            print("hidden_parameters_sentence")
            print(hidden_parameters_sentence)

            # TODO: Compute hidden_parameters_background
            hidden_parameters_background = ['todo']

            print(30*'*', 'Done one sentence')

        # Example on sparse dot product
        np.save(self.dump_path + "MY_HP_Updated", m)
        np.save(self.dump_path + "MY_HPB_updated", m)

#    def em_e_step_sparse_to_delete(self):
#        topic_model_sentence = reviews_binary[sentence].reshape(nw, 1).multiply(
#            topic_model)  ##TODO: can be extracted from here in prep part for all sentences#
#
#
#        for sentence in range(0, m):
#            sentence_sum = topic_model_sentence.dot(pi[sentence])
#            sentence_sum = np.where(sentence_sum == 0, 1, sentence_sum)
#            hidden_parameters_sentence = ((topic_model_sentence.multiply(pi[sentence])).T / sentence_sum).T
#            hidden_parameters_background = ['todo'] # TODO


if __name__ == '__main__':
    em = EM()
    em.em_prepare_data_for_testing()
    # em.em_e_step_dense()
    em.em_e_step_sparse()
